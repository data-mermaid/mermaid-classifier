---
title: Dask Preprocessing"
format: html
---

This notebook uses dask to preprocess the coralnet annotations.csv files and then uses duckdb to query the data. 
When using larger sources with many annotations, the preprocessing step can take a long time.
Instead, we can use dask locally in parallel to speed up these operations.

## Import libraries

```{python}
import pandas as pd
import dask.dataframe as dd
import dask.array as da
import dask.bag as db
import numpy as np
import boto3
import s3fs
import json
from botocore.exceptions import ClientError, BotoCoreError

```

Set up the locations on S3 and the local filesystem


```{python}
bucketname = 'coralnet-mermaid-share'
prefix = 'coralnet_public_features/'
try:
    # Load the secret.json file
    with open('secrets.json', 'r') as f:
        secrets = json.load(f)

    # Create a session using the credentials from secrets.json
    s3_client = boto3.client(
        's3',
        region_name=secrets['AWS_REGION'],
        aws_access_key_id=secrets['AWS_ACCESS_KEY_ID'],
        aws_secret_access_key=secrets['AWS_SECRET_ACCESS_KEY']
    )
except (ClientError, BotoCoreError) as e:
    print(f"An AWS error occurred: {e}")
except json.JSONDecodeError as e:
    print(f"Error reading secrets.json: {e}")
except IOError as e:
    print(f"File error: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

```

We need a paginator to list all the folders in the bucket under the prefix
```{python}
def list_folders(s3_client, bucketname, prefix):
    """
    List all folders in an S3 bucket under a specific prefix.

    Args:
        s3_client (boto3.client): The S3 client.
        bucketname (str): The name of the S3 bucket.
        prefix (str): The prefix (folder path).

    Returns:
        list: A list of folder names.

    Example:
        s3_client = boto3.client('s3', region_name='us-west-2')
        bucketname = 'my-bucket'
        prefix = 'my-folder/'
        folders = list_folders(s3_client, bucketname, prefix)
        print(folders)
    """
    paginator = s3_client.get_paginator('list_objects_v2')
    folders = []
    for page in paginator.paginate(Bucket=bucketname, Delimiter='/', Prefix=prefix):
        for prefix in page.get('CommonPrefixes', []):
            folders.append(prefix['Prefix'])
    return folders


folders = list_folders(s3_client, bucketname, prefix)

```

The sources to keep are derived from WCS work on identifying sources that are suitable for training a classifier.
```{python}
sources_to_keep = pd.read_csv('list_of_sources.csv') 
```

Output the list of folders to a csv file

```{python}
# Write out list of folders / sources as a csv file
df = pd.DataFrame(folders, columns=['source_folder'])
df.to_csv('folders.csv', index=False)
```

```{python}
df = pd.read_csv('folders.csv')
```
```{python}
# Get the source from source_folder column
df['source'] = df['source_folder'].str.split('/').str[1]
```

## Choose the sources to use

This will use all of the sources, alternatively you can specify a list of sources to use.
```{python}
# Convert the source column to a list
sources = df['source'].tolist()
#sources = ['s1970', 's2083', 's2170']
```


## Find the annotations.csv files for the chosen sources

-   Create the key value from the source

```{python}
# Create a list for the chosen sources for the annotations f'coralnet_public_features/{source}/annotations.csv'
chosen_sources = []
for source in sources:
    chosen_sources.append(f'coralnet_public_features/{source}/annotations.csv')
```

Using s3fs to access S3 objects quickly

```{python}
dfs = []
try:
    # Load the secret.json file
    with open('secrets.json', 'r') as f:
        secrets = json.load(f)

    # Create an S3FileSystem instance with the credentials from secrets.json
    s3 = s3fs.S3FileSystem(
        key=secrets['AWS_ACCESS_KEY_ID'],
        secret=secrets['AWS_SECRET_ACCESS_KEY'],
        anon = False
    )

    # Use s3 to access S3 objects
    bucket_name = 'coralnet-mermaid-share'
    missing_sources = []

    for source in chosen_sources:
        if s3.exists(f'{bucket_name}/{source}'):
            
            print(f"{source} exists in the bucket.")
            df = dd.read_csv(f's3://{bucket_name}/{source}', storage_options={'key': secrets['AWS_ACCESS_KEY_ID'], 'secret': secrets['AWS_SECRET_ACCESS_KEY']})

            df['source_id'] = source.split('/')[1]
            dfs.append(df)
        else:
            missing_sources.append(source)
            print(f"{source} does not exist in the bucket.")
except (ClientError, BotoCoreError) as e:
    print(f"An AWS error occurred: {e}")
except json.JSONDecodeError as e:
    print(f"Error reading secrets.json: {e}")
except IOError as e:
    print(f"File error: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
```

```{python}
# Get list of annotations.csv keys/paths for each source from S3
# Combine / Append all annotations.csv files into one dataframe
# Save as CoralNet_Annotations_Export 
# Create a Dask DataFrame by reading annotations.csv files from S3

# Append the Dask DataFrames vertically
merged_df = dd.concat(dfs, axis=0, ignore_index=True)

```

```{python}
# Number of rows in merged_df
merged_df.shape[0].compute()
```

## Save output as parquet files for smaller file storage and faster read/write

```{python}
# Write out the merged_df as a parquet file
merged_df.to_parquet('CoralNet_Annotations_SourceID.parquet', engine='pyarrow')
```

# Approach 1: Using Dask

```{python}
from dask.distributed import Client
client = Client()
```

```{python}
# Read in the previous parquet file
merged_df = dd.read_parquet('CoralNet_Annotations_SourceID.parquet', engine='pyarrow')
```

## Calculate stats

```{python}
# Group by Label ID and count the number of annotations for each label
label_counts = merged_df.groupby('Label ID').count().compute()
```

## Approach 2: DuckDB method

Let's try using duckdb instead of dask


```{python}
import os
import duckdb

# Check current working directory
cwd = os.getcwd()
print("Current Working Directory:", cwd)

# List files in the current directory
print("Files in the directory:", os.listdir(cwd))

# Full path to the file
parquet_file = os.path.join(cwd, 'CoralNet_Annotations_SourceID.parquet')

# Check if the file exists
if os.path.exists(parquet_file):
    print("File found, proceeding with query...")
    query = f"SELECT * FROM '{parquet_file}'"
    result = duckdb.query(query).fetchall()
    print(result)
else:
    print("File not found. Please check the file path.")
```


```{python}
import pandas as pd
import duckdb
cwd = os.getcwd()
parquet_file = os.path.join(cwd, 'CoralNet_Annotations_SourceID.parquet')

# Read the Parquet file into a Pandas DataFrame
df = pd.read_parquet(parquet_file)

# Connect to DuckDB
conn = duckdb.connect()

# Create a DuckDB table from the DataFrame
duckdb_df = conn.from_df(df)

```


Since we have this in duckdb let's restructure some of the training code in hopes that it also benefits from significant speedups in pre-processing.

## Rework labels 

- pyspacer takes in labels as tuples of (row, column, label_id) 
- We can restructure the data to be in this format and then use duckdb to query the data


```{python}
# Filter duckdb_df to only include the source_ids that are in sources_to_keep 
duckdb_df = conn.execute(
    f'SELECT * FROM duckdb_df WHERE "source_id" IN {tuple(sources_to_keep["source_id"].tolist())}'
).fetchdf()
```
```{python}
duckdb_df = conn.execute("""
    SELECT *, 
    'coralnet_public_features/' || CAST(source_id AS VARCHAR) || '/features/i' || CAST("Image ID" AS VARCHAR) || '.featurevector' AS key
    FROM duckdb_df
""").fetch_df()
```


```{python}
# Group by the Label ID and count the number of annotations for each label using duckdb
result = conn.execute(
    'SELECT "Label ID", COUNT(*) FROM duckdb_df GROUP BY "Label ID"').fetchall()
```

- 2253 Label IDs from coralnet

Count the number of labels for each Label ID

```{python}
# Save the result as a csv file
df = pd.DataFrame(result, columns=['Label ID', 'Count'])
df.to_csv('CoralNet_Label_ID_Count.csv', index=False)
```

Count the number of labels by source

```{python}
result2 = conn.execute(
    'SELECT "source_id", "Label ID", COUNT(*) FROM duckdb_df GROUP BY "source_id", "Label ID"'
).fetchall()
```

```{python}
result3 =  conn.execute(
    'SELECT "source_id", COUNT(*) as count FROM duckdb_df GROUP BY "source_id" ORDER BY count DESC'
).fetchdf()
```

```{python}
result2.len()
```

## Training using duckdb 

```{python}
total_labels = len(duckdb_df)
train_size = int(total_labels * 7 / 8 )

# Use duckdb to create train_labels_data and val_labels_data
train_labels_data = conn.execute(
    f'SELECT * FROM duckdb_df LIMIT {train_size}'
).fetchdf()

val_labels_data = conn.execute(
    f'SELECT * FROM duckdb_df OFFSET {train_size} ROWS'
).fetchdf()
```

```{python}
train_labels_data = conn.execute(
    f'SELECT * FROM duckdb_df LIMIT {train_size}'
).fetchdf()
```

Below, the could is a bit more inefficient to parse to this format
Suggest using dataframes as input in the future
```{python}
# Rewrite 
train_labels_data = {f"{key}": [tuple(x) for x in group[['Row', 'Column', 'Label ID']].values]
                     for key, group in train_labels_data.groupby('key')}

```


```{python}
val_labels_data = {f"{key}": [tuple(x) for x in group[['Row', 'Column', 'Label ID']].values]
                   for key, group in val_labels_data.groupby('key')}
```

```{python}

import json
import boto3
from botocore.exceptions import ClientError, BotoCoreError, NoCredentialsError
import os
import pickle
import pandas as pd
from datetime import datetime, timedelta
import io
import csv
import logging
import traceback
from operator import itemgetter
from pathlib import Path
from spacer import config
from spacer.data_classes import ImageLabels
from scripts.docker import runtimes
from spacer.tasks import (
    process_job,
    classify_image,
    extract_features,
    train_classifier,
    classify_features,
)
from spacer.storage import load_image, store_image
from spacer.extract_features import EfficientNetExtractor
from spacer.messages import (
    DataLocation,
    ExtractFeaturesMsg,
    ExtractFeaturesReturnMsg,
    TrainClassifierMsg,
    TrainClassifierReturnMsg,
    ClassifyFeaturesMsg,
    ClassifyImageMsg,
    ClassifyReturnMsg,
    JobMsg,
    JobReturnMsg,
)

from spacer.tasks import classify_features, extract_features, train_classifier

```

```{python}
train_msg = TrainClassifierMsg(
    job_token='mulitest',
    trainer_name='minibatch',
    nbr_epochs=1,
    clf_type='MLP',
    # A subset
    train_labels=ImageLabels(data = train_labels_data),
    val_labels=ImageLabels(data = val_labels_data),
    #S3 bucketname
    features_loc=DataLocation('s3', bucketname = bucketname, key=''),
    previous_model_locs=[],
    model_loc=DataLocation('filesystem',str(Path.cwd())+'/classifier_all_source.pkl'),
    valresult_loc=DataLocation('filesystem',str(Path.cwd())+'/valresult_all_source.json'),

)
```

```{python}
%%time
return_msg = train_classifier(train_msg)
```

```{python}
print(f"Train time: {return_msg.runtime:.1f} s")

```
```{python}
fileObj = open('return_msg_all.obj', 'wb')
pickle.dump(return_msg,fileObj)
fileObj.close()
```